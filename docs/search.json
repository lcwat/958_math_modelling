[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "watson-hw-7",
    "section": "",
    "text": "The goal of this exercise is to practice running bootstrapping in R with the boot package.\n\n\nHere are the libraries and data I will be using:\n\nlibrary(tidyverse) # cleaning\nlibrary(boot) # for running bootstraps\nlibrary(lme4) # mixed effect modelling and glmer\nlibrary(broom.mixed) # tidy model reports for lmer and glmer\n\n\n\n\nI created and ran four bootstrapped statistics for two models. The main effects only model using assault, urban population, and rape per 100,000 to predict murder per 100,000 using the statewide arrest data from the 1970s. The full factorial model included each of these same main effects and all of their interactions. For each model, I used either 25 or 10,000 samples to bootstrap the R-squared, resulting in 4 bootstraps in total.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1 - Plot showing smoothed density of bootstrap R-squared estimates for each model and bootstrap sample size. Dashed lines represent t0, or the original estimate for each.\nWith more samples, the distributions begin to approximate normality (as expected by the central limit theorem). The full factorial distributions also seem to lean more to the right of the dashed line (original R-squared) as negative skew, which could represent consistent overfitting of the data in the bootstrap samples.\nI also plotted the confidence intervals generated for the bootstrapped R-squared statistics for each of the models sampled 10,000 times.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2 - Comparison of bootstrapped R-squared estimates for each model with 10,000 bootstrap samples. Error bars represent 95% C.I.\n\n\n\n\n\n\nComparison of model results for each of the four bootstraps ran.\n\n\n\nmodel\nsamples\nmean R-squared\nt\np\n\n\n\n\nmain effects only\n25\n.69\n-4.17\n&lt;.001\n\n\nfull factorial\n25\n.75\n\n\n\n\nmain effects only\n10000\n.69\n-79.56\n&lt;.001\n\n\nfull factorial\n10000\n.76\n\n\n\n\n\n\n\n\n\nAfter creating these bootstrapped statistics, I have come to the conclusion that the full factorial model is not my preferred model. As expected, the R-squared statistic is significantly larger in the full factorial model compared to the main effects only model for small and large bootstrapped samples (see Table 1). Simply explaining more variance as a function of having more predictors is not that impressive.\nWith more samples, the bootstrapping procedure becomes more confident in its estimates. This is evidenced in the reduced size of the confidence interval (Figure 2) and the results of the t-test (Table 1). On another note, the confidence intervals generated by the bca with boot.ci() are asymmetrical, with longer interval in the LL, slightly more so for the full factorial model. This may be due to the fact that the R-squared can’t stretch further for the UL, running into the ceiling of one and squishing any potential variation in that direction. Although the tendency for R-squared to favor overfitted models could also impact the shape of the distributions (Figure 1) and the resulting bootstrapped confidence intervals.\n\n\n\nUsing the battalion dataset, I conducted a mixed effects model using the lmer command from the lme4 library. I predicted combat readiness using rank, age, rank * age interaction, and time as fixed effects with random intercepts. The results are summarized in Table 2.\n\n\nParameter estimates for the mixed effects model predicting readiness.\n\n\n\nestimate\nB\nSE\nt\n\n\n\n\nintercept\n3.04\n.03\n88.32\n\n\nrank\n.05\n.01\n4.41\n\n\nage\n.01\n.01\n1.17\n\n\ntime\n.10\n.02\n4.74\n\n\nrank * age\n.01\n.00\n3.38\n\n\n\nNote. Each predictor was mean-centered prior to analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 - Bootstrapped samples fixed effects estimates for the mixed effects model (excluding intercept). The black error bars represent +/- 2 S.E. generated by the original model fit, and the green error bars represent the 95% C.I. generated by the bootstrapped samples.\n\n\n\nThe mixed effects model returned a few significant predictors, although the goal for this section was to probe the differences in interpretation returned from the fixed effect error estimates between the standard errors and a bootstrapped 95% confidence interval for the same fixed effect. Although there are slight differences, like the C.I. being slightly smaller for each estimate, I am not really sure what the advantage of this approach would be. I feel like the standard errors that lmer spits out are good enough in this instance. Perhaps it could help with plotting, creating error ribbons that represent the C.I. rather than the estimation of the C.I. using +/- 2 standard errors could be beneficial for visualization and clarity.\n\n\n\n\nI used similar syntax with boot to create confidence intervals for statistics that usually do not have those available due to some lacking theoretical formulas for their calculation. I will run bootstrapped samples of standard deviation and median rape per 100,000 by state in the USArrests dataset.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4 - Histogram of bootstrapped standard deviation values for rape per 100,000 in the USArrests dataset.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5 - Original standard deviation of the USArrests dataset with bootstrapped 95% C.I. errorbars.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6 - Histogram of bootstrapped median values for rape per 100,000 from USArrests dataset.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7 - Original value of median with bootstrapped 95% C.I. for rape per 100,000 in USArrests dataset.\n\n\n\nAfter bootstrapping both the standard deviation and median statistics for this sample, there were some immediate differences that stood out to me. The histogram of bootstrapped values for the median is strange. It seems positively skewed but also has some weird poisson like characteristics of peaks at certain values and gaps for others. This statistic is likely more constrained than the standard deviation. The median must pull a value from the dataset at hand, leading to consistent sampling of the same values. The standard deviation is not constrained in the same way, it simply calculates a value from each new bootstrap sample with no regard for what values it must output.\nThe documentation for boot says it uses ran.gen to index data, but I’m not exactly sure how this could change the shape or why there is more variation on the lower end below the dashed line in Figure 6. It could be due to the skewed shape of the original distribution. If the larger values are nixed when indexing, it is more likely then to be using the lower half of the dataset. Since there are more small values though, if these get nixed during indexing, it likely won’t push the bootstrap estimate up as much. So, the result is more samples of smaller values.\n\n\n\n\nI watched a video the other day where this Ph.D. student was at a conference and decided to track the amount of times a speaker “umm-ed” during their talk (‘I found a weird pattern in how people UHM’ by NotDavid on YouTube). They used a clicker to track the amount of umms as well as how much time elapsed between them (inter-umm time or IUT). Using this data, I ran a generalized mixed effects model using a poisson error distribution with fixed effects of professional science communicator status (professional speaker or not), centered time of their speech, and the interaction with random intercepts. The model was run using the glmer command from the lme4 library. The results are summarized in Table 3.\n\n\n\n\n\nestimate\nB\nSE\nt\np\n\n\n\n\nintercept\n4.339\n.102\n42.72\n&lt;.001\n\n\nprofessional speaker\n.376\n.101\n3.71\n&lt;.001\n\n\nminutes spoke\n.009\n.005\n1.60\n.110\n\n\ninteraction\n.002\n.005\n.45\n.650\n\n\n\nNote - Model fixed effect estimates for poisson generalized mixed effects model predicting “umms”. Estimates and errors are log-transformed counts of “umms.” Minutes spoke was a centered continuous predictor and professional speaker was an effect-coded, 2-level categorical variable.\n\n\n\nLike previous examples, I also bootstrapped the fixed effects estimates for this model.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8 - Comparison of fixed effect estimates measure of error with +/- 2 S.E. in black and 95% bootstrapped C.I. in green.\n\n\n\nAfter running the model, I found some significant results (Table 3). As expected, being a professional speaker resulted in fewer “umms” being predicted (B = .376, SE = .101, bootstrapped 95% C.I. = [.188, .545], p &lt; .001). Unexpectedly, the amount of minutes spoken for did not result in a significant difference in “umms” (B = .009, SE = .005, bootstrapped 95% C.I. = [-.001, .017], p = .110), and the interaction between professional status and minutes spoken was not significant (B = .002, SE = .005, bootstrapped 95% C.I. = [-.009, .011], p = .650). I would have expected that with more time, the speaker would have more chances to “umm.” Perhaps those speaking for longer stretches of time at a conference or talk feel inclined to rehearse more and therefore are less likely to “err” or “umm” during their time."
  },
  {
    "objectID": "index.html#exercise-7",
    "href": "index.html#exercise-7",
    "title": "watson-hw-7",
    "section": "",
    "text": "The goal of this exercise is to practice running bootstrapping in R with the boot package.\n\n\nHere are the libraries and data I will be using:\n\nlibrary(tidyverse) # cleaning\nlibrary(boot) # for running bootstraps\nlibrary(lme4) # mixed effect modelling and glmer\nlibrary(broom.mixed) # tidy model reports for lmer and glmer\n\n\n\n\nI created and ran four bootstrapped statistics for two models. The main effects only model using assault, urban population, and rape per 100,000 to predict murder per 100,000 using the statewide arrest data from the 1970s. The full factorial model included each of these same main effects and all of their interactions. For each model, I used either 25 or 10,000 samples to bootstrap the R-squared, resulting in 4 bootstraps in total.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1 - Plot showing smoothed density of bootstrap R-squared estimates for each model and bootstrap sample size. Dashed lines represent t0, or the original estimate for each.\nWith more samples, the distributions begin to approximate normality (as expected by the central limit theorem). The full factorial distributions also seem to lean more to the right of the dashed line (original R-squared) as negative skew, which could represent consistent overfitting of the data in the bootstrap samples.\nI also plotted the confidence intervals generated for the bootstrapped R-squared statistics for each of the models sampled 10,000 times.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2 - Comparison of bootstrapped R-squared estimates for each model with 10,000 bootstrap samples. Error bars represent 95% C.I.\n\n\n\n\n\n\nComparison of model results for each of the four bootstraps ran.\n\n\n\nmodel\nsamples\nmean R-squared\nt\np\n\n\n\n\nmain effects only\n25\n.69\n-4.17\n&lt;.001\n\n\nfull factorial\n25\n.75\n\n\n\n\nmain effects only\n10000\n.69\n-79.56\n&lt;.001\n\n\nfull factorial\n10000\n.76\n\n\n\n\n\n\n\n\n\nAfter creating these bootstrapped statistics, I have come to the conclusion that the full factorial model is not my preferred model. As expected, the R-squared statistic is significantly larger in the full factorial model compared to the main effects only model for small and large bootstrapped samples (see Table 1). Simply explaining more variance as a function of having more predictors is not that impressive.\nWith more samples, the bootstrapping procedure becomes more confident in its estimates. This is evidenced in the reduced size of the confidence interval (Figure 2) and the results of the t-test (Table 1). On another note, the confidence intervals generated by the bca with boot.ci() are asymmetrical, with longer interval in the LL, slightly more so for the full factorial model. This may be due to the fact that the R-squared can’t stretch further for the UL, running into the ceiling of one and squishing any potential variation in that direction. Although the tendency for R-squared to favor overfitted models could also impact the shape of the distributions (Figure 1) and the resulting bootstrapped confidence intervals.\n\n\n\nUsing the battalion dataset, I conducted a mixed effects model using the lmer command from the lme4 library. I predicted combat readiness using rank, age, rank * age interaction, and time as fixed effects with random intercepts. The results are summarized in Table 2.\n\n\nParameter estimates for the mixed effects model predicting readiness.\n\n\n\nestimate\nB\nSE\nt\n\n\n\n\nintercept\n3.04\n.03\n88.32\n\n\nrank\n.05\n.01\n4.41\n\n\nage\n.01\n.01\n1.17\n\n\ntime\n.10\n.02\n4.74\n\n\nrank * age\n.01\n.00\n3.38\n\n\n\nNote. Each predictor was mean-centered prior to analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 - Bootstrapped samples fixed effects estimates for the mixed effects model (excluding intercept). The black error bars represent +/- 2 S.E. generated by the original model fit, and the green error bars represent the 95% C.I. generated by the bootstrapped samples.\n\n\n\nThe mixed effects model returned a few significant predictors, although the goal for this section was to probe the differences in interpretation returned from the fixed effect error estimates between the standard errors and a bootstrapped 95% confidence interval for the same fixed effect. Although there are slight differences, like the C.I. being slightly smaller for each estimate, I am not really sure what the advantage of this approach would be. I feel like the standard errors that lmer spits out are good enough in this instance. Perhaps it could help with plotting, creating error ribbons that represent the C.I. rather than the estimation of the C.I. using +/- 2 standard errors could be beneficial for visualization and clarity.\n\n\n\n\nI used similar syntax with boot to create confidence intervals for statistics that usually do not have those available due to some lacking theoretical formulas for their calculation. I will run bootstrapped samples of standard deviation and median rape per 100,000 by state in the USArrests dataset.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4 - Histogram of bootstrapped standard deviation values for rape per 100,000 in the USArrests dataset.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5 - Original standard deviation of the USArrests dataset with bootstrapped 95% C.I. errorbars.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6 - Histogram of bootstrapped median values for rape per 100,000 from USArrests dataset.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7 - Original value of median with bootstrapped 95% C.I. for rape per 100,000 in USArrests dataset.\n\n\n\nAfter bootstrapping both the standard deviation and median statistics for this sample, there were some immediate differences that stood out to me. The histogram of bootstrapped values for the median is strange. It seems positively skewed but also has some weird poisson like characteristics of peaks at certain values and gaps for others. This statistic is likely more constrained than the standard deviation. The median must pull a value from the dataset at hand, leading to consistent sampling of the same values. The standard deviation is not constrained in the same way, it simply calculates a value from each new bootstrap sample with no regard for what values it must output.\nThe documentation for boot says it uses ran.gen to index data, but I’m not exactly sure how this could change the shape or why there is more variation on the lower end below the dashed line in Figure 6. It could be due to the skewed shape of the original distribution. If the larger values are nixed when indexing, it is more likely then to be using the lower half of the dataset. Since there are more small values though, if these get nixed during indexing, it likely won’t push the bootstrap estimate up as much. So, the result is more samples of smaller values.\n\n\n\n\nI watched a video the other day where this Ph.D. student was at a conference and decided to track the amount of times a speaker “umm-ed” during their talk (‘I found a weird pattern in how people UHM’ by NotDavid on YouTube). They used a clicker to track the amount of umms as well as how much time elapsed between them (inter-umm time or IUT). Using this data, I ran a generalized mixed effects model using a poisson error distribution with fixed effects of professional science communicator status (professional speaker or not), centered time of their speech, and the interaction with random intercepts. The model was run using the glmer command from the lme4 library. The results are summarized in Table 3.\n\n\n\n\n\nestimate\nB\nSE\nt\np\n\n\n\n\nintercept\n4.339\n.102\n42.72\n&lt;.001\n\n\nprofessional speaker\n.376\n.101\n3.71\n&lt;.001\n\n\nminutes spoke\n.009\n.005\n1.60\n.110\n\n\ninteraction\n.002\n.005\n.45\n.650\n\n\n\nNote - Model fixed effect estimates for poisson generalized mixed effects model predicting “umms”. Estimates and errors are log-transformed counts of “umms.” Minutes spoke was a centered continuous predictor and professional speaker was an effect-coded, 2-level categorical variable.\n\n\n\nLike previous examples, I also bootstrapped the fixed effects estimates for this model.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8 - Comparison of fixed effect estimates measure of error with +/- 2 S.E. in black and 95% bootstrapped C.I. in green.\n\n\n\nAfter running the model, I found some significant results (Table 3). As expected, being a professional speaker resulted in fewer “umms” being predicted (B = .376, SE = .101, bootstrapped 95% C.I. = [.188, .545], p &lt; .001). Unexpectedly, the amount of minutes spoken for did not result in a significant difference in “umms” (B = .009, SE = .005, bootstrapped 95% C.I. = [-.001, .017], p = .110), and the interaction between professional status and minutes spoken was not significant (B = .002, SE = .005, bootstrapped 95% C.I. = [-.009, .011], p = .650). I would have expected that with more time, the speaker would have more chances to “umm.” Perhaps those speaking for longer stretches of time at a conference or talk feel inclined to rehearse more and therefore are less likely to “err” or “umm” during their time."
  },
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "hello.html#meet-quarto",
    "href": "hello.html#meet-quarto",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "hello.html#meet-the-penguins",
    "href": "hello.html#meet-the-penguins",
    "title": "Hello, Quarto",
    "section": "Meet the penguins",
    "text": "Meet the penguins\n\nThe penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\nThe plot below shows the relationship between flipper and bill lengths of these penguins."
  }
]